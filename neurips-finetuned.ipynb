{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":56167,"databundleVersionId":6535361,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 📦 Cell 1: Importing Libraries and Setting Device\nIn this cell, we import necessary libraries, check for GPU availability, and set the device accordingly.","metadata":{}},{"cell_type":"code","source":"import os  # 📂 Import the 'os' module to interact with the operating system.\nimport subprocess  # 🚀 Import the 'subprocess' module for running external commands.\n\nimport pandas as pd  # 📊 Import 'pandas' library for data manipulation.\nimport torch  # 🔥 Import 'torch' library for PyTorch functionalities.\nimport torchvision  # 🖼️ Import 'torchvision' for computer vision utilities.\nimport torch.nn as nn  # 🧠 Import 'nn' module from 'torch' for neural network components.\nimport torch.optim as optim  # ⚙️ Import 'optim' module for optimization algorithms.\nfrom torchvision.models import resnet18  # 📸 Import the ResNet-18 model from torchvision.\nfrom torch.utils.data import DataLoader, Dataset  # 🧾 Import data-related components.\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # 🧭 Check and set the device to GPU if available, otherwise use CPU.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-07T05:29:19.788222Z","iopub.execute_input":"2023-12-07T05:29:19.788581Z","iopub.status.idle":"2023-12-07T05:29:23.490880Z","shell.execute_reply.started":"2023-12-07T05:29:19.788549Z","shell.execute_reply":"2023-12-07T05:29:23.489926Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 🚧 Cell 2: Accelerator Check\nThis cell checks if a GPU accelerator is available and raises an error if not.","metadata":{}},{"cell_type":"code","source":"#use p100\nif DEVICE != 'cuda':\n    raise RuntimeError('❗ Make sure you have added an accelerator (e.g., GPU) to your notebook; the submission will fail otherwise! 🚀')","metadata":{"execution":{"iopub.status.busy":"2023-12-07T05:29:23.492771Z","iopub.execute_input":"2023-12-07T05:29:23.493178Z","iopub.status.idle":"2023-12-07T05:29:23.497758Z","shell.execute_reply.started":"2023-12-07T05:29:23.493152Z","shell.execute_reply":"2023-12-07T05:29:23.496817Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# 📂 Cell 3: Dataset Loading and Preparation\nHere, we define functions and classes for loading and preparing the dataset for training and validation.","metadata":{}},{"cell_type":"code","source":"def load_example(df_row):\n    '''Load an example from the dataset.'''\n    image = torchvision.io.read_image(df_row['image_path'])  # 📷 Read the image using torchvision.\n    result = {\n        'image': image,\n        'image_id': df_row['image_id'],\n        'age_group': df_row['age_group'],\n        'age': df_row['age'],\n        'person_id': df_row['person_id']\n    }\n    return result\n\n\nclass HiddenDataset(Dataset):\n    '''The hidden dataset for training and validation.'''\n    def __init__(self, split='train'):\n        super().__init__()\n        self.examples = []\n\n        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')  # 📊 Read the dataset CSV.\n        df['image_path'] = df['image_id'].apply(\n            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n        df = df.sort_values(by='image_path')  # 📂 Sort the dataset by image path.\n        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n        if len(self.examples) == 0:\n            raise ValueError('No examples.')\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        '''Get an example from the dataset.'''\n        example = self.examples[idx]\n        image = example['image']\n        image = image.to(torch.float32)  # 🌟 Convert image to float32.\n        example['image'] = image\n        return example\n\n\ndef get_dataset(batch_size):\n    '''Get dataloaders for different dataset splits.'''\n    retain_ds = HiddenDataset(split='retain')\n    forget_ds = HiddenDataset(split='forget')\n    val_ds = HiddenDataset(split='validation')\n\n    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)  # 📦 Create a DataLoader for 'retain' split.\n    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)  # 📦 Create a DataLoader for 'forget' split.\n    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)  # 📦 Create a DataLoader for 'validation' split.\n\n    return retain_loader, forget_loader, validation_loader","metadata":{"execution":{"iopub.status.busy":"2023-12-07T05:29:23.499128Z","iopub.execute_input":"2023-12-07T05:29:23.499397Z","iopub.status.idle":"2023-12-07T05:29:23.518985Z","shell.execute_reply.started":"2023-12-07T05:29:23.499374Z","shell.execute_reply":"2023-12-07T05:29:23.518182Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 🧠 Cell 4: Unlearning Function\nThis cell contains the unlearning function, which fine-tunes the model using a provided dataset.","metadata":{}},{"cell_type":"code","source":"def unlearning(\n    net, \n    retain_loader, \n    forget_loader, \n    val_loader):\n    '''Simple unlearning by finetuning.'''\n    epochs = 1\n    criterion = nn.CrossEntropyLoss()  # 📉 Define the loss function.\n    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)  # 🚀 Define the optimizer.\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # 📉 Learning rate scheduler.\n    net.train()  # 🏋️ Set the network to training mode.\n\n    for ep in range(epochs):\n        net.train()  # 🏋️ Set the network to training mode.\n        for sample in retain_loader:\n            inputs = sample[\"image\"]\n            targets = sample[\"age_group\"]\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)  # 🧭 Move data to the selected device (GPU or CPU).\n        \n            optimizer.zero_grad()  # 🧹 Zero the gradients.\n            outputs = net(inputs)  # 🧠 Forward pass.\n            loss = criterion(outputs, targets)  # 📉 Calculate the loss.\n            loss.backward()  # ⏪ Backpropagate the gradients.\n            optimizer.step()  # 🚀 Update model parameters.\n        scheduler.step()  # 📉 Adjust learning rate using the scheduler.\n        \n    net.eval()  # 🧪 Set the network to evaluation mode when done training.","metadata":{"execution":{"iopub.status.busy":"2023-12-07T05:29:23.520106Z","iopub.execute_input":"2023-12-07T05:29:23.520419Z","iopub.status.idle":"2023-12-07T05:29:23.532707Z","shell.execute_reply.started":"2023-12-07T05:29:23.520390Z","shell.execute_reply":"2023-12-07T05:29:23.531996Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 💾 Cell 5: Checkpoint Generation & Submission\nIn this cell, we create unlearned model checkpoints and ensure there are exactly 512 checkpoints.and handles the submission process, including creating the submission.zip file.","metadata":{}},{"cell_type":"code","source":"if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n    # 📦 Mock submission: Create an empty submission.zip file.\n    subprocess.run('touch submission.zip', shell=True)\nelse:\n    # 🚀 Create unlearned checkpoints outside of the working directory to avoid disk space issues.\n    os.makedirs('/kaggle/tmp', exist_ok=True)\n\n    # 🧾 Load the datasets and initialize the model.\n    retain_loader, forget_loader, validation_loader = get_dataset(64)\n    net = resnet18(weights=None, num_classes=10)\n    net.to(DEVICE)\n\n    # 🔄 Iterate to create unlearned checkpoints.\n    for i in range(512):\n        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))  # 📂 Load the original model.\n        unlearning(net, retain_loader, forget_loader, validation_loader)  # 🧠 Perform unlearning.\n        state = net.state_dict()  # 📄 Get the model's state.\n        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')  # 💾 Save unlearned checkpoint.\n\n    # 📏 Check the number of unlearned checkpoints to ensure it's 512.\n    unlearned_ckpts = os.listdir('/kaggle/tmp')\n    if len(unlearned_ckpts) != 512:\n        raise RuntimeError('❗ Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n\n    # 📦 Create the submission.zip file containing the unlearned checkpoints.\n    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T05:29:23.534367Z","iopub.execute_input":"2023-12-07T05:29:23.534689Z","iopub.status.idle":"2023-12-07T05:29:23.554368Z","shell.execute_reply.started":"2023-12-07T05:29:23.534623Z","shell.execute_reply":"2023-12-07T05:29:23.553549Z"},"trusted":true},"execution_count":5,"outputs":[]}]}